{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# Kaggle Agents - Real Competition (Colab)\n",
    "\n",
    "Este notebook executa o `kaggle-agents` em **competicoes reais do Kaggle**, baixando dados via API, gerando submissions automaticamente, e submetendo ao leaderboard.\n",
    "\n",
    "Workflow:\n",
    "1. Download dos dados via Kaggle API\n",
    "2. Execucao autonoma do pipeline (search, plan, develop, ensemble, submit)\n",
    "3. Auto-submit da `submission.csv` ao Kaggle\n",
    "4. Visualizacao do score no leaderboard\n",
    "\n",
    "Pre-requisitos (Colab Secrets):\n",
    "- `OPENAI_API_KEY` ou `GEMINI_API_KEY` (dependendo do provider)\n",
    "- `KAGGLE_USERNAME`\n",
    "- `KAGGLE_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1) Setup (clone + install)\n",
    "\n",
    "Clona o repositorio e instala as dependencias. Nao precisa do `mle-bench`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_cell"
   },
   "outputs": [],
   "source": [
    "# GPU (opcional)\n",
    "!nvidia-smi || echo \"No GPU available (CPU mode)\"\n",
    "\n",
    "# Clone repo\n",
    "!test -d /content/kaggle-agents || git clone https://github.com/gustavogomespl/kaggle-agents.git /content/kaggle-agents\n",
    "\n",
    "%cd /content/kaggle-agents\n",
    "!ls -la | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Instalar deps (sem mle-bench)\n",
    "!pip -q install uv\n",
    "!uv pip install --system -e /content/kaggle-agents\n",
    "\n",
    "import sys\n",
    "print('python:', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "## 2) Configuracao (.env + Kaggle credentials)\n",
    "\n",
    "Configure no Colab -> **Secrets**:\n",
    "- `OPENAI_API_KEY` (ou `GEMINI_API_KEY`)\n",
    "- `KAGGLE_USERNAME`\n",
    "- `KAGGLE_KEY`\n",
    "\n",
    "Opcional (para controle de custo/qualidade):\n",
    "- `LLM_MODEL`, `PLANNER_MODEL`, `DEVELOPER_MODEL`, `EVALUATOR_MODEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "# ===== LLM config (ajuste aqui) =====\n",
    "os.environ['LLM_PROVIDER'] = os.environ.get('LLM_PROVIDER', 'gemini')\n",
    "os.environ['LLM_MODEL'] = os.environ.get('LLM_MODEL', 'gemini-3-flash-preview')\n",
    "os.environ['LLM_TEMPERATURE'] = os.environ.get('LLM_TEMPERATURE', '0')\n",
    "os.environ['LLM_MAX_TOKENS'] = os.environ.get('LLM_MAX_TOKENS', '16000')\n",
    "\n",
    "# Per-role overrides (opcional)\n",
    "os.environ['PLANNER_MODEL'] = os.environ.get('PLANNER_MODEL', os.environ['LLM_MODEL'])\n",
    "os.environ['DEVELOPER_MODEL'] = os.environ.get('DEVELOPER_MODEL', os.environ['LLM_MODEL'])\n",
    "os.environ['EVALUATOR_MODEL'] = os.environ.get('EVALUATOR_MODEL', os.environ['LLM_MODEL'])\n",
    "\n",
    "# ===== Secrets =====\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') or ''\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY') or ''\n",
    "kaggle_username = userdata.get('KAGGLE_USERNAME') or ''\n",
    "kaggle_key = userdata.get('KAGGLE_KEY') or ''\n",
    "\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    raise ValueError('Missing KAGGLE_USERNAME/KAGGLE_KEY (configure nos Colab Secrets).')\n",
    "\n",
    "# Kaggle credentials\n",
    "kaggle_path = Path.home() / '.kaggle' / 'kaggle.json'\n",
    "kaggle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "kaggle_path.write_text(json.dumps({'username': kaggle_username, 'key': kaggle_key}))\n",
    "kaggle_path.chmod(0o600)\n",
    "\n",
    "# .env\n",
    "env_path = Path('/content/kaggle-agents/.env')\n",
    "env_path.write_text(\n",
    "    \"\\n\".join([\n",
    "        f\"LLM_PROVIDER={os.environ['LLM_PROVIDER']}\",\n",
    "        f\"LLM_MODEL={os.environ['LLM_MODEL']}\",\n",
    "        f\"LLM_TEMPERATURE={os.environ['LLM_TEMPERATURE']}\",\n",
    "        f\"LLM_MAX_TOKENS={os.environ['LLM_MAX_TOKENS']}\",\n",
    "        f\"PLANNER_MODEL={os.environ['PLANNER_MODEL']}\",\n",
    "        f\"DEVELOPER_MODEL={os.environ['DEVELOPER_MODEL']}\",\n",
    "        f\"EVALUATOR_MODEL={os.environ['EVALUATOR_MODEL']}\",\n",
    "        f\"OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\",\n",
    "        f\"KAGGLE_USERNAME={kaggle_username}\",\n",
    "        f\"KAGGLE_KEY={kaggle_key}\",\n",
    "        f\"GOOGLE_API_KEY={os.environ['GOOGLE_API_KEY']}\",\n",
    "        \"LOG_LEVEL=INFO\",\n",
    "        \"LOG_DIR=./logs\",\n",
    "    ]) + \"\\n\"\n",
    ")\n",
    "\n",
    "print('kaggle.json:', kaggle_path)\n",
    "print('.env:', env_path)\n",
    "print('LLM_MODEL:', os.environ['LLM_MODEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "langsmith_cell"
   },
   "outputs": [],
   "source": [
    "# LangSmith Tracing (opcional)\n",
    "print(\"Configuring LangSmith Tracing...\")\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    langchain_api_key = userdata.get('LANGSMITH')\n",
    "    langsmith_project = userdata.get('LANGSMITH_PROJECT')\n",
    "    print(\"Loaded secrets from Colab\")\n",
    "except Exception:\n",
    "    langchain_api_key = None\n",
    "    langsmith_project = None\n",
    "\n",
    "if langchain_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = langsmith_project or \"kaggle-agents\"\n",
    "    print(f\"LangSmith Tracing Enabled (Project: {os.environ['LANGCHAIN_PROJECT']})\")\n",
    "\n",
    "    if os.path.exists(\".env\"):\n",
    "        with open(\".env\", \"a\") as f:\n",
    "            f.write(f\"\\nLANGCHAIN_TRACING_V2=true\\n\")\n",
    "            f.write(f\"LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\\n\")\n",
    "            f.write(f\"LANGCHAIN_API_KEY={langchain_api_key}\\n\")\n",
    "            f.write(f\"LANGCHAIN_PROJECT={os.environ['LANGCHAIN_PROJECT']}\\n\")\n",
    "else:\n",
    "    print(\"LangSmith Tracing Disabled (no LANGSMITH secret found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "competition_header"
   },
   "source": [
    "## 3) Competicao\n",
    "\n",
    "Defina o slug da competicao Kaggle. Exemplos:\n",
    "- `titanic` (Getting Started, classificacao binaria)\n",
    "- `house-prices-advanced-regression-techniques` (Getting Started, regressao)\n",
    "- `digit-recognizer` (Getting Started, classificacao multi-classe)\n",
    "- `spaceship-titanic` (Getting Started, classificacao binaria)\n",
    "\n",
    "**Importante:** Aceite as regras da competicao no site do Kaggle antes de rodar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "competition_params"
   },
   "outputs": [],
   "source": [
    "COMPETITION = \"titanic\"  # slug da competicao Kaggle\n",
    "MAX_ITERATIONS = 3\n",
    "TIMEOUT = 3000  # timeout por componente em segundos\n",
    "\n",
    "OUT_DIR = f\"/content/kaggle_results/{COMPETITION}\"\n",
    "\n",
    "print(f\"Competition: {COMPETITION}\")\n",
    "print(f\"Max iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"Timeout: {TIMEOUT}s\")\n",
    "print(f\"Output: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_workflow"
   },
   "outputs": [],
   "source": [
    "# Rodar workflow + auto-submit\n",
    "!KAGGLE_AGENTS_TARGET_SCORE=0.99 \\\n",
    "KAGGLE_AGENTS_MAX_COMPONENT_RETRIES=3 \\\n",
    "KAGGLE_AGENTS_COMPONENT_TIMEOUT_S=$TIMEOUT \\\n",
    "TESTING_TIMEOUT=$TIMEOUT \\\n",
    "TIMEOUT_MODEL_HEAVY=$TIMEOUT \\\n",
    "KAGGLE_AGENTS_MAX_COMPONENTS=6 \\\n",
    "KAGGLE_AGENTS_ENABLE_LIMITS=false \\\n",
    "KAGGLE_AGENTS_CV_FOLDS=5 \\\n",
    "python3 notebooks/kaggle_eval.py \\\n",
    "  --competition $COMPETITION \\\n",
    "  --max-iterations $MAX_ITERATIONS \\\n",
    "  --timeout $TIMEOUT \\\n",
    "  --auto-submit \\\n",
    "  -o $OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_results"
   },
   "outputs": [],
   "source": [
    "# Ver resultados\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_path = Path(OUT_DIR) / \"results.json\"\n",
    "summary_path = Path(OUT_DIR) / \"summary.json\"\n",
    "\n",
    "if results_path.exists():\n",
    "    results = json.loads(results_path.read_text())\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for r in results:\n",
    "        print(f\"\\nCompetition: {r.get('competition_id')}\")\n",
    "        print(f\"  Success: {r.get('success')}\")\n",
    "        print(f\"  Submitted: {r.get('submitted')}\")\n",
    "        print(f\"  Public Score: {r.get('public_score')}\")\n",
    "        print(f\"  Execution Time: {r.get('execution_time', 0):.1f}s\")\n",
    "        print(f\"  Iterations: {r.get('iterations')}\")\n",
    "        print(f\"  Components: {r.get('components_implemented')}\")\n",
    "        if r.get('error'):\n",
    "            print(f\"  Error: {r.get('error')[:200]}\")\n",
    "else:\n",
    "    print(f\"No results found at {results_path}\")\n",
    "\n",
    "if summary_path.exists():\n",
    "    summary = json.loads(summary_path.read_text())\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_leaderboard"
   },
   "outputs": [],
   "source": [
    "# Ver leaderboard e posicao\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = os.environ.get('KAGGLE_USERNAME', '')\n",
    "os.environ['KAGGLE_KEY'] = os.environ.get('KAGGLE_KEY', '')\n",
    "\n",
    "try:\n",
    "    from kaggle_agents.tools.kaggle_api import KaggleAPIClient\n",
    "    client = KaggleAPIClient()\n",
    "\n",
    "    # My submissions\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MY SUBMISSIONS - {COMPETITION}\")\n",
    "    print(\"=\" * 60)\n",
    "    submissions = client.get_my_submissions(COMPETITION)\n",
    "    for i, sub in enumerate(submissions[:5]):\n",
    "        print(f\"  [{i+1}] Score: {sub.get('publicScore')} | \"\n",
    "              f\"Status: {sub.get('status')} | \"\n",
    "              f\"Date: {sub.get('date')} | \"\n",
    "              f\"File: {sub.get('fileName')}\")\n",
    "\n",
    "    # Leaderboard top 10\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"LEADERBOARD TOP 10 - {COMPETITION}\")\n",
    "    print(\"=\" * 60)\n",
    "    leaderboard = client.get_leaderboard(COMPETITION, top_n=10)\n",
    "    for entry in leaderboard:\n",
    "        print(f\"  #{entry['rank']:>4} | {entry['score']:>10} | {entry['teamName']}\")\n",
    "\n",
    "    # My best score vs leaderboard\n",
    "    if submissions:\n",
    "        best_score = max(\n",
    "            (s.get('publicScore', 0) for s in submissions if s.get('status') == 'complete'),\n",
    "            default=None,\n",
    "        )\n",
    "        if best_score is not None and leaderboard:\n",
    "            total_teams = len(client.get_leaderboard(COMPETITION, top_n=10000))\n",
    "            rank_estimate = sum(1 for e in leaderboard if e['score'] >= best_score)\n",
    "            percentile = (1 - rank_estimate / max(total_teams, 1)) * 100\n",
    "            print(f\"\\nMy best score: {best_score}\")\n",
    "            print(f\"Estimated rank: ~{rank_estimate}/{total_teams}\")\n",
    "            print(f\"Estimated percentile: ~{percentile:.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch leaderboard: {e}\")"
   ]
  }
 ]
}
