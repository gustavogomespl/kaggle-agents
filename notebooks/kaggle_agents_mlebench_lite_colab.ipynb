{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Agents + MLE-bench Lite (Colab)\n",
        "\n",
        "Este notebook executa o `kaggle-agents` em competições do **MLE-bench Lite** e valida as submissões com `mlebench grade-sample`.\n",
        "\n",
        "Objetivo:\n",
        "- Rodar uma competição (ou o Lite) no MLE-bench\n",
        "- Gerar `results.json`, `summary.json`, `results.csv`\n",
        "- Comparar com o benchmark via `any_medal_percentage` (Low/Lite)\n",
        "\n",
        "Pré-requisitos (Colab Secrets):\n",
        "- `OPENAI_API_KEY`\n",
        "- `KAGGLE_USERNAME`\n",
        "- `KAGGLE_KEY`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup (clone + install)\n",
        "\n",
        "Este notebook assume os paths:\n",
        "- `/content/kaggle-agents`\n",
        "- `/content/mle-bench`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU (opcional)\n",
        "!nvidia-smi || echo \"No GPU available (CPU mode)\"\n",
        "\n",
        "# Clone repos (requer rede)\n",
        "!test -d /content/kaggle-agents || git clone https://github.com/gustavogomespl/kaggle-agents.git /content/kaggle-agents\n",
        "!test -d /content/mle-bench || git clone https://github.com/openai/mle-bench.git /content/mle-bench\n",
        "\n",
        "%cd /content/kaggle-agents\n",
        "!ls -la | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar deps\n",
        "!pip -q install uv\n",
        "!uv pip install --system -e /content/kaggle-agents\n",
        "!pip -q install -e /content/mle-bench\n",
        "\n",
        "import sys\n",
        "print('python:', sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Configuração (.env + Kaggle credentials)\n",
        "\n",
        "Configure no Colab → **Secrets**:\n",
        "- `OPENAI_API_KEY`\n",
        "- `KAGGLE_USERNAME`\n",
        "- `KAGGLE_KEY`\n",
        "\n",
        "Opcional (para controle de custo/qualidade):\n",
        "- `LLM_MODEL`, `PLANNER_MODEL`, `DEVELOPER_MODEL`, `EVALUATOR_MODEL`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# ===== LLM config (ajuste aqui) =====\n",
        "os.environ['LLM_PROVIDER'] = os.environ.get('LLM_PROVIDER', 'openai')\n",
        "os.environ['LLM_MODEL'] = os.environ.get('LLM_MODEL', 'gpt-5-mini')\n",
        "os.environ['LLM_TEMPERATURE'] = os.environ.get('LLM_TEMPERATURE', '0.7')\n",
        "os.environ['LLM_MAX_TOKENS'] = os.environ.get('LLM_MAX_TOKENS', '16000')\n",
        "\n",
        "# Per-role overrides (opcional)\n",
        "os.environ['PLANNER_MODEL'] = os.environ.get('PLANNER_MODEL', os.environ['LLM_MODEL'])\n",
        "os.environ['DEVELOPER_MODEL'] = os.environ.get('DEVELOPER_MODEL', os.environ['LLM_MODEL'])\n",
        "os.environ['EVALUATOR_MODEL'] = os.environ.get('EVALUATOR_MODEL', os.environ['LLM_MODEL'])\n",
        "\n",
        "# ===== Secrets =====\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') or ''\n",
        "kaggle_username = userdata.get('KAGGLE_USERNAME') or ''\n",
        "kaggle_key = userdata.get('KAGGLE_KEY') or ''\n",
        "\n",
        "if not os.environ['OPENAI_API_KEY']:\n",
        "    raise ValueError('Missing OPENAI_API_KEY (configure nos Colab Secrets).')\n",
        "if not kaggle_username or not kaggle_key:\n",
        "    raise ValueError('Missing KAGGLE_USERNAME/KAGGLE_KEY (configure nos Colab Secrets).')\n",
        "\n",
        "# Kaggle credentials para o CLI do mlebench\n",
        "kaggle_path = Path.home() / '.kaggle' / 'kaggle.json'\n",
        "kaggle_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "kaggle_path.write_text(json.dumps({'username': kaggle_username, 'key': kaggle_key}))\n",
        "kaggle_path.chmod(0o600)\n",
        "\n",
        "# .env (útil para processos subprocess / reexecuções)\n",
        "env_path = Path('/content/kaggle-agents/.env')\n",
        "env_path.write_text(\n",
        "    \"\\n\".join([\n",
        "        f\"LLM_PROVIDER={os.environ['LLM_PROVIDER']}\",\n",
        "        f\"LLM_MODEL={os.environ['LLM_MODEL']}\",\n",
        "        f\"LLM_TEMPERATURE={os.environ['LLM_TEMPERATURE']}\",\n",
        "        f\"LLM_MAX_TOKENS={os.environ['LLM_MAX_TOKENS']}\",\n",
        "        f\"PLANNER_MODEL={os.environ['PLANNER_MODEL']}\",\n",
        "        f\"DEVELOPER_MODEL={os.environ['DEVELOPER_MODEL']}\",\n",
        "        f\"EVALUATOR_MODEL={os.environ['EVALUATOR_MODEL']}\",\n",
        "        f\"OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\",\n",
        "        f\"KAGGLE_USERNAME={kaggle_username}\",\n",
        "        f\"KAGGLE_KEY={kaggle_key}\",\n",
        "        \"KAGGLE_AUTO_SUBMIT=false\",\n",
        "        \"LOG_LEVEL=INFO\",\n",
        "        \"LOG_DIR=./logs\",\n",
        "    ]) + \"\\n\"\n",
        ")\n",
        "\n",
        "print('✅ kaggle.json:', kaggle_path)\n",
        "print('✅ .env:', env_path)\n",
        "print('✅ LLM_MODEL:', os.environ['LLM_MODEL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Preparar datasets (MLE-bench)\n",
        "\n",
        "Dica:\n",
        "- Primeiro rode 1 competição para validar o pipeline.\n",
        "- Depois rode `--lite` se quiser o Lite completo (22 competições).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste rápido (1 competição)\n",
        "!mlebench prepare -c aerial-cactus-identification\n",
        "\n",
        "# Lite completo (22 competições) - pode demorar\n",
        "# !mlebench prepare --lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Rodar avaliação (gera `results.json`, `summary.json`, `results.csv`)\n",
        "\n",
        "Dica para economizar tempo/custo:\n",
        "- Comece com `--max-iterations 1` e `--timeout 1200` (20 min por componente)\n",
        "- Depois aumente gradualmente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "RUN_ID = 'aerial_cactus_smoke'\n",
        "OUT_DIR = Path(f'/content/mlebench_results/{RUN_ID}')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# IMPORTANT: use path absoluto (evita erro '/content/notebooks/...')\n",
        "!python3 /content/kaggle-agents/notebooks/mlebench_eval.py --competition aerial-cactus-identification --max-iterations 1 --timeout 1800 -o {OUT_DIR}\n",
        "\n",
        "# Lite completo (opcional)\n",
        "# !python3 /content/kaggle-agents/notebooks/mlebench_eval.py --lite --max-iterations 1 --timeout 1800 -o /content/mlebench_results/lite_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "out_dir = Path(f'/content/mlebench_results/{RUN_ID}')\n",
        "summary = json.loads((out_dir / 'summary.json').read_text())\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n",
        "print('\\n=== Benchmark (MLE-bench) ===')\n",
        "print(f\"Low/Lite Any Medal (%): {summary.get('any_medal_percentage', 0.0) * 100:.2f}%\")\n",
        "print(f\"Valid Submission (%): {summary.get('valid_submission_percentage', 0.0) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) (Opcional) Repetir 3 runs e reportar mean ± SEM\n",
        "\n",
        "O leaderboard do MLE-bench reporta `Any Medal (%)` como **média ± SEM** (recomenda 3 seeds/runs).\n",
        "\n",
        "Abaixo roda o Lite 3 vezes e calcula a estatística. Ajuste `--max-iterations`/`--timeout` conforme seu orçamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json, math, statistics\n",
        "from pathlib import Path\n",
        "\n",
        "RUN_GROUP = 'lite_3runs'\n",
        "BASE_DIR = Path('/content/mlebench_results') / RUN_GROUP\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "seeds = [0, 1, 2]\n",
        "summaries = []\n",
        "\n",
        "for seed in seeds:\n",
        "    out_dir = BASE_DIR / f'seed_{seed}'\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # (Opcional) logar seed para rastreio\n",
        "    os.environ['RUN_SEED'] = str(seed)\n",
        "    \n",
        "    # Rodar Lite completo\n",
        "    !python3 /content/kaggle-agents/notebooks/mlebench_eval.py --lite --max-iterations 1 --timeout 1800 -o {out_dir}\n",
        "    summaries.append(json.loads((out_dir / 'summary.json').read_text()))\n",
        "\n",
        "any_medals = [s.get('any_medal_percentage', 0.0) for s in summaries]\n",
        "mean = statistics.mean(any_medals)\n",
        "sem = (statistics.stdev(any_medals) / math.sqrt(len(any_medals))) if len(any_medals) > 1 else 0.0\n",
        "\n",
        "print(f\"\\nLite Any Medal (%): {mean*100:.2f} ± {sem*100:.2f} (n={len(any_medals)})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
